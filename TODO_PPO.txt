TODO:
-- logging:
- log actor / critic loss + l2 loss (and generally the other losses properly too)
- log average advantage / reward / return too
- log average gradient norm and clipping factor used
- remove ep_scores, #episodes variable and scores length / ... instead
- log current learning rate...

-- general:
- optimizer cfg choosing
- add l2 loss for value network
- choose scale or clip action space (and implement scaling with mean + magnitudes)
- define calculate gae properly, use two fcts or smth (for returns / advantages)
- remove TODO from calulate gae
- give _train the variables y_true_actions/returns + y_pred_vest_old IN their parameter
- 158-167 can we do that more elegant? through some wrapper function and functionals?
- rename ppo_entropy_factor to entropy_factor
- add 0.5 from gradient to cfg
- put distribution stuff in _util !
- is action rescaling actually implemented properly? since actions range != action_dist range...
- norm to logpdf maybe
- multi environment learning (should be easy with the vecenv...)

-- rest:
- costa.sh -> implementation details checken
- use forced types in parameters dosmth(epsilon: float = 1e-4, ...)
- proper naming for envs in tb!
- pull out all seeds to config such that it's possible to reproduce results exactly
- document optimizations ... (and give sources of each :))